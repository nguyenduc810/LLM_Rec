{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:06:48.707045: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 02:06:48.753722: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 02:06:49.565520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-21 02:06:50.583372: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os \n",
    "from pprint import pprint\n",
    "import json \n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(idir):\n",
    "    if not os.path.isdir(idir):\n",
    "        os.makedirs(idir)\n",
    "\n",
    "def get_noun_phrases(doc, output=None, keep=None):\n",
    "    if keep is None:\n",
    "        return list([np.text.lower() for np in doc.noun_chunks])\n",
    "    if output is None:\n",
    "        output = {}\n",
    "    kws = []\n",
    "    for nc in doc.noun_chunks:\n",
    "        ws = []\n",
    "        for word in nc:\n",
    "            if word.pos_ in keep:\n",
    "                ws.append(word.text.lower())\n",
    "        if len(ws) > 0:\n",
    "            n = ' '.join(ws)\n",
    "            output[n] = output.get(n, 0) + 1\n",
    "            kws.append(n)\n",
    "    return output, kws \n",
    "\n",
    "def increase_count(idict, key, freq):\n",
    "    if key not in idict:\n",
    "        idict[key] = 0\n",
    "    idict[key] += freq\n",
    "\n",
    "\n",
    "def add_to_dict(idict, key, value, freq=1):\n",
    "    if key not in idict:\n",
    "        idict[key] = {}\n",
    "    if value not in idict[key]:\n",
    "        idict[key][value] = 0\n",
    "    idict[key][value] += freq\n",
    "\n",
    "\n",
    "def get_unique_values(idict, count_only=False):\n",
    "    if count_only:\n",
    "        return {k: len(set(v)) for k, v in idict.items()}\n",
    "    else:\n",
    "        return {k: list(set(v)) for k, v in idict.items()}\n",
    "\n",
    "\n",
    "def save_np_info(np2count, np2reviews, np2rest, np2users, ofile, count_only=False):\n",
    "    # output = {\"np2count\": np2count, \"np2review_count\": count_unique_values(np2reviews), \n",
    "    #         'np2res_count': count_unique_values(np2rest), 'np2user_count': count_unique_values(np2users)}\n",
    "    output = {\"np2count\": np2count, \"np2reviews\": np2reviews, \n",
    "            'np2rests': np2rest, 'np2users': np2users}\n",
    "    json.dump(output, open(ofile, 'w'))\n",
    "    print(\"Saved to\", ofile)\n",
    "\n",
    "def extract_raw_keywords_for_reviews(data, ofile, \n",
    "                                     keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], overwrite=False, \n",
    "                                     review2keyword_ofile=None):\n",
    "    if os.path.isfile(ofile) and not overwrite:\n",
    "        print(\"Existing output file. Stop! (set overwrite=True to overwrite)\")\n",
    "        return \n",
    "    np2count = {}   # frequency \n",
    "    np2review2count = {}  # reviews \n",
    "    np2rest2count = {}  # \n",
    "    np2user2count = {} \n",
    "    counter = 0\n",
    "    review2keywords = {}\n",
    "    for rid, uid, restid, text in tqdm(zip(data['review_id'], data['user_id'], data['rest_id'], data['text']), total=len(data)):\n",
    "        doc = nlp(text)\n",
    "        tmp, keywords = get_noun_phrases(doc, keep=keep)  # np for this review \n",
    "        for np, freq in tmp.items():\n",
    "            increase_count(np2count, np, freq)\n",
    "            add_to_dict(np2review2count, np, rid, freq)\n",
    "            add_to_dict(np2rest2count, np, restid, freq)\n",
    "            add_to_dict(np2user2count, np, uid, freq)\n",
    "        review2keywords[rid] = keywords\n",
    "        # counter += 1\n",
    "        # if counter % 2 == 0:\n",
    "            # save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    save_np_info(np2count, np2review2count, np2rest2count, np2user2count, ofile)\n",
    "    if review2keyword_ofile is not None: \n",
    "        df = pd.DataFrame({\"Review_ID\": list(review2keywords.keys()), \"Keywords\": list(review2keywords.values())})\n",
    "        df.to_csv(review2keyword_ofile)\n",
    "\n",
    "\n",
    "def load_split(sfile='/home/ubuntu/duc.nm195858/keyext_LLM/splits.json', city='singapore', setname='train'):\n",
    "    return json.load(open(sfile))[city][setname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keywords for train set\n",
    "- Extract keywords from train set \n",
    "- Then use postprocessing for choosing a subset of keywords \n",
    "- Use these keywords for test, dev? (maybe don't need, but can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for charlotte train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90426/90426 [27:50<00:00, 54.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/charlotte-keywords.json\n",
      "Processing for edinburgh train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10342/10342 [03:47<00:00, 45.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/edinburgh-keywords.json\n",
      "Processing for lasvegas train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343524/343524 [1:45:14<00:00, 54.40it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/lasvegas-keywords.json\n",
      "Processing for london train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33990/33990 [11:39<00:00, 48.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/london-keywords.json\n",
      "Processing for phoenix train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216488/216488 [1:01:10<00:00, 58.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/phoenix-keywords.json\n",
      "Processing for pittsburgh train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73558/73558 [22:25<00:00, 54.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/pittsburgh-keywords.json\n",
      "Processing for singapore train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10615/10615 [03:53<00:00, 45.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/train/singapore-keywords.json\n"
     ]
    }
   ],
   "source": [
    "# setname = 'test'\n",
    "CITIES = ['charlotte', 'edinburgh', 'lasvegas', 'london', 'phoenix', 'pittsburgh', 'singapore']\n",
    "sets = ['train']#, 'test', 'dev']\n",
    "for city in CITIES: \n",
    "    dt = pd.read_csv('/home/ubuntu/duc.nm195858/keyext_LLM/reviews/{}.csv'.format(city))\n",
    "    for setname in sets: \n",
    "        print(\"Processing for\", city, setname)\n",
    "        uids = load_split(city=city, setname=setname)\n",
    "        dt_set = dt[dt['user_id'].isin(uids)]\n",
    "        odir = '/home/ubuntu/duc.nm195858/keyext_LLM/preprocessed/by_city-users_min_3_reviews/keywords_spacy/' + setname\n",
    "        mkdir(odir)\n",
    "        extract_raw_keywords_for_reviews(dt_set, ofile=os.path.join(odir, city + '-keywords.json'), keep=['ADJ', 'NOUN', 'PROPN', 'VERB'], \n",
    "                                        overwrite=True, review2keyword_ofile=os.path.join(odir,city+\"-review2keywords.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
